version: '3'
x-spark-common:
  &spark-common
  build: 
    context: ./image_spark 
    labels:
      cluster: spark
    args:
      &spark-common-version
      SPARK_VERSION: 3.1.2
      HADOOP_VERSION: 3.2
  entrypoint: ["/bin/bash", "/app/start-cluster.sh"]
  environment:
    &spark-common-env
    SPARK_HOME: /opt/spark
x-spark-worker-common:
  &spark-worker-common
  <<: *spark-common  # inherited from "spark-common", but not if key existed
  depends_on:
    - base
    - spark-master
  volumes:
    - ./mounted_dirs/data:/app/data  # mock distributed files, mounted on jupyter & worker
  environment:
    &spark-worker-common-env
    <<: *spark-common-env  # i.e. SPARK_HOME: /opt/spark
    SPARK_WORKER_CORES: 2
    SPARK_WORKER_MEMORY: 2G
    SPARK_MASTER: spark://spark-master:7077  # refer to service "spark-master"

services:
  # prepare base image
  base:
    image: local/spark-base
    build: 
      context: ./image_base
      labels:
        cluster: spark
    command: ["echo", "...build base image for Spark clsuter..."]
  # prepare python archived dependencies to mounted directory
  pyspark-venv:
    depends_on: 
      - base
    build: 
      context: ./image_venv
      labels:
        cluster: spark 
    volumes:
      - ./mounted_dirs/jobs:/app/jobs
    command: ["cp", "/app/pyspark_venv.tar.gz", "/app/jobs/"]
  # container of Jupyter server
  jupyter-server: 
    build: 
      context: ./image_jupyter
      labels:
        cluster: spark
      args:
        <<: *spark-common-version  # i.e. match Pyspark version with Spark
        USER_UID: 1000
        USER_NAME: $USER # current user on host machine
    depends_on: 
      - base
      - pyspark-venv
    stdin_open: true # docker run -i
    tty: true        # docker run -t
    volumes:
      - ./mounted_dirs/data:/app/data
      - ./mounted_dirs/notebooks:/app/notebooks
      - ./mounted_dirs/jobs:/app/jobs
    ports: 
      - target: 8888  # container
        published: 8888  # host
        protocol: tcp
        mode: host
      - 4040:4040 # web UI (started by a SparkContext, can be 4041,4042...)
  # container for cluster master node
  spark-master:
    <<: *spark-common
    depends_on: 
      - base
    volumes:
      - ./mounted_dirs/data:/app/data
    ports:
      - 8080:8080 # host:container, web UI for master process
      - 7077:7077 # exposed to submit job from host machine
    expose: 
      - 7077 # master port exposed to cluster machines
    environment:
      <<: *spark-common-env
      SPARK_TYPE: master # variable for script, as the master in cluster
      SPARK_MASTER_PORT: 7077
      SPARK_MASTER_WEBUI_PORT: 8080
  # container for cluster worker node
  spark-worker-1:
    <<: *spark-worker-common
    ports:
      - 8081:8081 # worker1
    environment:
      <<: *spark-worker-common-env
      SPARK_WORKER_WEBUI_PORT: 8081
  # container for cluster worker node
  spark-worker-2:
    <<: *spark-worker-common
    ports:
      - 8082:8082 # worker2
    environment:
      <<: *spark-worker-common-env
      SPARK_WORKER_WEBUI_PORT: 8082
  # container for for database
  postgres-db: 
    profiles:  # skip starting by default
      - donotstart
    image: postgres:9.6
    environment: 
      POSTGRES_USER: someUser
      POSTGRES_PASSWORD: somePassword
      POSTGRES_DB: someDB
    volumes:
      - database-data:/var/lib/postgresql/data/
volumes: 
  database-data:
